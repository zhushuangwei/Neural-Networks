set.seed(1)
x1<-c(4,1,3)
x2<-c(2,3,4)
x3<-c(7,8,1)
x4<-c(3,4,5)
x5<-c(2,4,5)
x<-cbind(x1,x2,x3,x4,x5)#3*5
y<-matrix(c(1,0,1,1,0,
            1,1,0,1,0),nrow = 2,byrow = T)
#设置神经网络结构 
Layer_num<-c(3,4,2)
#设置学习速率
eta=0.5
#s型神经元 
sigmod=function(x){
    return(1/(1+exp(-x)))
}
sigmod_prime<-function(x){
    return(sigmod(x)*(1-sigmod(x)))
}
w1<-matrix(rnorm(Layer_num[1]*Layer_num[2],0,1),nrow = Layer_num[1],byrow = T)
b1<-rnorm(Layer_num[2],0,1)
w2<-matrix(rnorm(Layer_num[2]*Layer_num[3],0,1),nrow = Layer_num[2],byrow = T)
b2<-rnorm(Layer_num[3],0,1)
z1<-t(w1)%*%x+b1
a1=sigmod(z1)
z2<-t(w2)%*%a1+b2
a2<-sigmod(z2)
a2
C<-sum((y-a2)**2)/2
#反向传播最后一层
dz2=(a2-y)*sigmod_prime(z2)
dw2<-a1%*%t(dz2)
db2<-dz2
#反向传播倒数第二层
dz1=w2%*%dz2*sigmod_prime(z1)
dw1=x%*%t(dz1)
db1=dz1

n=0
#梯度下降 
w1=w1-eta*dw1
w2=w2-eta*dw2
b1=b1-eta*db1
b2=b2-eta*db2
loss=c()
#设置循环 
while(C>0.001){
    z1<-t(w1)%*%x+b1
    a1<-sigmod(z1)
    z2<-t(w2)%*%a1+b2
    a2=sigmod(z2)
    dz2=(a2-y)*sigmod_prime(z2)
    dw2<-a1%*%t(dz2)
    db2<-dz2
    dz1=w2%*%dz2*sigmod_prime(z1)
    dw1=x%*%t(dz1)
    db1=dz1
    C<-sum((y-a2)**2)/2
    w1=w1-eta*dw1
    w2=w2-eta*dw2
    b1=b1-eta*db1
    b2=b2-eta*db2
    n=n+1
    loss[n]=C
}
plot(loss)
z1<-t(w1)%*%x+b1
a1=sigmod(z1)
z2<-t(w2)%*%a1+b2
a2<-sigmod(z2)
a2
y
n
C

